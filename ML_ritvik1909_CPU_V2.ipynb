{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição do Problema\n",
    "\n",
    "Classificação de documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuração do ambiente de trabablho\n",
    "\n",
    "- no container docker é instalado o tesseract-ocr\n",
    "- instalando e importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytesseract: É uma biblioteca em Python que serve como uma interface para o Tesseract OCR Engine. O Tesseract é uma ferramenta de código aberto muito utilizada para realizar OCR em imagens.\n",
    "# %pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------SKLEARN--------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score # ,auc,roc_curve,precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split # , GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "\n",
    "# from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer,ClusteringScoreVisualizer\n",
    "# from sklearn.metrics import silhouette_score\n",
    "\n",
    "#-------NLTK--------------\n",
    "# NLTK (Natural Language Toolkit) para processamento de linguagem natural (PLN)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from unidecode import unidecode\n",
    "from contractions import fix\n",
    "# from nltk.util import ngrams\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer #, LancasterStemmer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#-------TENSORFLOW---------\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Input, SimpleRNN, LSTM , GRU, Bidirectional, Embedding\n",
    "# from keras.layers import Dropout\n",
    "\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# from keras.utils import to_categorical\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pytesseract\n",
    "import re\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from xgboost import XGBClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coleta de Dados\n",
    "\n",
    "- Dataset do kaggle: https://www.kaggle.com/datasets/ritvik1909/document-classification-dataset\n",
    "\n",
    "- Baixar os dados e armazenar em \"dataset/www-kaggle-com_datasets_ritvik1909_document-classification-dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Exploratória de Dados (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo de e-mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset/www-kaggle-com_datasets_ritvik1909_document-classification-dataset/email/doc_000042.png\"\n",
    "image = Image.open(path)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo de Resumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset/www-kaggle-com_datasets_ritvik1909_document-classification-dataset/resume/doc_000051.png\"\n",
    "image = Image.open(path)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo de publicação cientifica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset/www-kaggle-com_datasets_ritvik1909_document-classification-dataset/scientific_publication/doc_000016.png\"\n",
    "image = Image.open(path)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraindo dados de dentro da imagem via OCR\n",
    "\n",
    "### Realizar OCR (Reconhecimento Óptico de Caracteres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai textos da imagem\n",
    "text = pytesseract.image_to_string(image)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpeza de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lista de palavras que são consideradas stopwords (palavras vazias) para o idioma inglês. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK (Natural Language Toolkit) \n",
    "# conjunto de palavras stopwords para o idioma inglês \n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "stopwords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparação dos Dados para a modelagem\n",
    "\n",
    "### Exemplo de processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversão para Minúsculas \n",
    "\n",
    "Substituição de Quebras de Linha e Tabulações \n",
    "\n",
    "Remoção de Espaços Extras \n",
    "\n",
    "Remoção de Números\n",
    "\n",
    "Remoção de Pontuação \n",
    "\n",
    "Tokenização (word_tokenize(text)):\n",
    " - word_tokenize(text) utiliza a função word_tokenize do NLTK para dividir o texto em tokens individuais (palavras e pontuações). Isso é fundamental para trabalhar com o texto em nível de palavra.\n",
    "Remoção de Pontuação Adicional e Stopwords\n",
    "\n",
    "Lemmatização:\n",
    "- A lematização é realizada usando o WordNetLemmatizer do NLTK. Cada palavra restante após a remoção de stopwords é lematizada, ou seja, reduzida à sua forma base (lema). Isso ajuda na normalização do texto, reduzindo variações morfológicas e simplificando o vocabulário.\n",
    "\n",
    "Concatenação dos Tokens:\n",
    "- todos os tokens lematizados são unidos em uma única string, onde cada token é separado por um espaço em branco. Esta string processada é então retornada como a saída da função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\n\",\" \").replace(\"\\t\",\" \")\n",
    "    text = re.sub(\"\\s+\",\" \",text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # tokens\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    data = [i for i in tokens if i not in punctuation]\n",
    "    data = [i for i in data if i not in stopwords_list]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    final_text = []\n",
    "    for i in data:\n",
    "        word = lemmatizer.lemmatize(i)\n",
    "        final_text.append(word)\n",
    "        \n",
    "    return \" \".join(final_text)\n",
    "\n",
    "preprocess_data(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import pyarrow.parquet as pq\n",
    "\n",
    "# Definição de Classes, opções de target\n",
    "class_labels = {'email':0,'resume':1,'scientific_publication':2}\n",
    "\n",
    "# usada para armazenar os textos extraídos das imagens após o pré-processamento\n",
    "final_text = []\n",
    "# usada para armazenar os rótulos numéricos correspondentes a cada documento\n",
    "final_label = []\n",
    "path = \"dataset/www-kaggle-com_datasets_ritvik1909_document-classification-dataset\"\n",
    "image_folder = os.listdir(path)\n",
    "\n",
    "# Verificar se o arquivo Parquet já existe, para não precisar reprocessar as imagem novamente\n",
    "parquet_file = \"document_texts.parquet\"\n",
    "if os.path.exists(parquet_file):\n",
    "    # Carregar dados existentes se o arquivo Parquet já existir\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    final_text = df['Text'].tolist()\n",
    "    final_label = df['Label'].tolist()\n",
    "else:\n",
    "    # para cada imagem do dataset\n",
    "    for label_dir in image_folder:\n",
    "        # caminho completo para o diretório atual de documentos (por classe 'email', 'resume' e 'scientific_publication')\n",
    "        label_path = os.path.join(path,label_dir)\n",
    "        # Itera sobre os arquivos dentro do diretório \"\"\n",
    "        for j in os.listdir(label_path):\n",
    "            # carrega a imagem.png\n",
    "            image = Image.open(label_path+\"/\"+j)\n",
    "            # Usa Tesseract OCR para extrair o texto da imagem\n",
    "            text = pytesseract.image_to_string(image)\n",
    "            # executa preprocessamento\n",
    "            text_data = preprocess_data(text)\n",
    "            # Adiciona o texto pré-processado (text_data) à lista final_text\n",
    "            final_text.append(text_data)\n",
    "            # Adiciona o rótulo numérico correspondente ao tipo de documento (label) à lista final_label, usando o dicionário class_labels para fazer a correspondência entre nome do diretório e rótulo numérico.\n",
    "            final_label.append(class_labels[label_dir])\n",
    "            \n",
    "    # Criar um DataFrame com os resultados\n",
    "    df = pd.DataFrame({'Text': final_text, 'Label': final_label})\n",
    "    # Salvar DataFrame em arquivo Parquet\n",
    "    df.to_parquet(parquet_file, index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando Dataframe pandas com base nas lista 'final_text' e 'final_label'\n",
    "\n",
    "Coluna 'Text' representa o texto\n",
    "\n",
    "Coluna 'Label' tem valores 0, 1 e 2 (classes 'email', 'resume' e 'scientific_publication')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['Text'] = final_text\n",
    "df['Label'] = final_label\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'][110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'][110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisar classes do DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'].value_counts().plot(kind='pie',autopct=\"%.1f%%\")\n",
    "plt.title(\"Distribuição da quantidade de documentos\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df['Label'])\n",
    "plt.title(\"Document Distributions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividindo o dataframe em treino e teste (split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(df['Text'],df['Label'],shuffle=True,test_size=0.10)\n",
    "print(\"Quantidade para treino: \" + str(x_train.shape))\n",
    "print(\"Quantidade para teste: \" + str(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abordagem abaixo é útil para representar textos de forma numérica para serem usados como entrada em modelos de aprendizado de máquina, como classificadores ou modelos de clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O TfidfVectorizer do scikit-learn é uma ferramenta que converte uma coleção de documentos de texto em uma matriz de características TF-IDF (Term Frequency-Inverse Document Frequency).  Matriz esparsa, que é eficiente para armazenar dados quando a maioria dos valores são zeros. A matriz esparsa economiza memória ao armazenar apenas os índices e valores dos elementos não-zero. A matriz densa ocupa mais memória porque armazena todos os elementos, incluindo os zeros.\n",
    "\n",
    "\n",
    "- Criação de uma matriz TF-IDF para representar dados textuais. TF-IDF (Term Frequency-Inverse Document Frequency) é uma técnica para extrair características de texto que refletem a importância de uma palavra ou n-grama em relação a um documento em um corpus.\n",
    "\n",
    "- TfidfVectorizer: Esta é uma classe do sklearn.feature_extraction.text que converte uma coleção de documentos de texto em uma matriz TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "\n",
    "- ngram_range=(2,5): Define que serão considerados n-grams de tamanho mínimo 2 e máximo 5. Isso significa que o vetorizador criará características que são combinações de 2 até 5 palavras consecutivas.\n",
    "\n",
    "- max_df=0.95: Remove termos que aparecem em mais de 95% dos documentos. Isso ajuda a remover palavras muito comuns que não são discriminativas.\n",
    "\n",
    "- min_df=2: Remove termos que aparecem em menos de 2 documentos. Isso ajuda a remover palavras raras que podem não contribuir muito para o modelo.\n",
    "\n",
    "- max_features=10000: Limita o número máximo de características (palavras ou n-grams) a serem extraídas. Aqui, será limitado a 10.000 características com base na frequência TF-IDF mais alta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando o TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(2,5), max_df=0.95, min_df=2 ,max_features=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit_transform(x_train): Este método ajusta o vetorizador ao conjunto de dados de treino x_train e, em seguida, transforma x_train em uma matriz TF-IDF. Isso significa que ele aprende o vocabulário do conjunto de treinamento e transforma cada documento (no caso, cada texto em x_train) em uma representação numérica usando TF-IDF.\n",
    "\n",
    "A matriz resultante de tfidf.fit_transform(x_train) é uma matriz esparsa (tipicamente CSR - Compressed Sparse Row format), que é eficiente em termos de memória. Convertendo-a em uma matriz densa do NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando e transformando os dados de treinamento\n",
    "tfidf_train = tfidf.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform(x_test): transformando os dados de teste x_test em uma matriz TF-IDF usando o vocabulário aprendido durante o ajuste (fit_transform). É importante usar transform nos dados de teste, não fit_transform, porque queremos aplicar o mesmo vocabulário do conjunto de treinamento aos dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo a matriz esparsa em uma matriz densa\n",
    "tfidf_test = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train.A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matriz esparsa de TF-IDF.\n",
    "tfidf_train.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train.A[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construindo o Modelo\n",
    "\n",
    "Regressão Logistica\n",
    "\n",
    "utilizando a classe LogisticRegression do scikit-learn para treinar um modelo de regressão logística com os dados de treinamento que foram transformados usando o vetorizador TF-IDF.\n",
    "\n",
    "criando uma instância do modelo de Regressão Logística. A regressão logística é um algoritmo de aprendizado supervisionado usado para problemas de classificação binária e multiclasse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento do Modelo\n",
    "\n",
    "\n",
    "tfidf_train.A é a matriz de características de treinamento transformada usando TF-IDF. O método .A converte a matriz esparsa para uma matriz densa, que é esperada pelo método fit da regressão logística.\n",
    "\n",
    "y_train são os rótulos correspondentes aos exemplos de treinamento. Cada rótulo indica a classe à qual o exemplo pertence.\n",
    "\n",
    "O vetorizador TF-IDF (tfidf) foi ajustado aos dados de treinamento (x_train) antes deste treinamento, definindo o vocabulário e calculando os pesos TF-IDF para cada palavra ou conjunto de palavras nos documentos.\n",
    "\n",
    "tfidf_train.A é a matriz de características transformada usando TF-IDF. Ela contém representações numéricas das palavras presentes nos documentos de treinamento.\n",
    "\n",
    "model.fit(tfidf_train.A, y_train) usa essas representações para treinar o modelo de regressão logística, onde tfidf_train.A são as características de entrada e y_train são os rótulos correspondentes às classes dos documentos de treinamento.\n",
    "\n",
    "Após o treinamento, o modelo estará ajustado aos dados de treinamento e poderá ser usado para fazer previsões sobre novos dados (nesse caso, usando tfidf_test para transformar os dados de teste e model.predict para fazer previsões).\n",
    "\n",
    "Este procedimento faz parte do fluxo típico de trabalho em aprendizado de máquina supervisionado para classificação de texto utilizando o método TF-IDF em conjunto com a regressão logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tfidf_train.A, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previsões\n",
    "\n",
    "utilizando o modelo de regressão logística treinado (model) para fazer previsões sobre os dados de teste que foram transformados usando TF-IDF (tfidf_test).\n",
    "\n",
    "model.predict() é o método utilizado para fazer previsões com o modelo treinado.\n",
    "\n",
    "tfidf_test.A é a matriz de características de teste transformada usando TF-IDF. Assim como no treinamento, .A converte a matriz esparsa para uma matriz densa, que é esperada pelo método predict da regressão logística.\n",
    "\n",
    "\n",
    "Após treinar o modelo de regressão logística com os dados de treinamento, agora está usando o modelo para prever as classes dos dados de teste.\n",
    "\n",
    "tfidf_test.A contém as representações TF-IDF dos dados de teste, ou seja, como cada palavra nos documentos de teste foi transformada numericamente usando TF-IDF.\n",
    "\n",
    "model.predict(tfidf_test.A) aplica o modelo treinado aos dados de teste para prever a classe de cada exemplo de teste.\n",
    "\n",
    "O resultado das previsões é armazenado em y_pred, que é uma lista ou matriz (dependendo da forma de tfidf_test.A) contendo as classes previstas para os exemplos de teste.\n",
    "\n",
    "y_pred contém as classes previstas para os exemplos de teste. Cada valor em y_pred corresponde à classe prevista para o exemplo correspondente em tfidf_test.A.\n",
    "\n",
    "Com essas previsões, pode avaliar o desempenho do seu modelo comparando y_pred com os rótulos reais (y_test) dos dados de teste.\n",
    "\n",
    "Este é o passo final em muitos fluxos de trabalho de aprendizado de máquina supervisionado, onde após treinar o modelo, fazemos previsões sobre novos dados para avaliar a eficácia do modelo treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões com dados de test\n",
    "y_pred = model.predict(tfidf_test.A)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previsões com dados de treinamento para detectar overfitting\n",
    "\n",
    "utilizando o modelo de regressão logística treinado (model) para fazer previsões sobre os dados de treinamento que foram transformados usando TF-IDF (tfidf_train).\n",
    "\n",
    "Após treinar o modelo de regressão logística com os dados de treinamento, está usando o modelo para prever as classes dos próprios dados de treinamento.\n",
    "\n",
    "útil para verificar se o modelo está \"decorando\" os dados de treinamento, o que poderia indicar overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tr = model.predict(tfidf_train.A)\n",
    "y_pred_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Matriz de confusão:\\n\",confusion_matrix(y_test,y_pred))\n",
    "print()\n",
    "print(\"Relatório de Classificação:\\n\",classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix:\\n\",confusion_matrix(y_train,y_pred_tr))\n",
    "print()\n",
    "print(\"Classification Report:\\n\",classification_report(y_train,y_pred_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy on Test Data: {accuracy_score(y_test,y_pred)*100:.2f} %\")\n",
    "print(f\"Accuracy on Train Data: {accuracy_score(y_train,y_pred_tr)*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_model = LogisticRegression(max_iter=2000,solver='liblinear',C=100)\n",
    "lg_model.fit(tfidf_train.A,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lg_model.predict(tfidf_test.A)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n",
    "print()\n",
    "print(\"Classification Report:\\n\",classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy on Test Data: {accuracy_score(y_test,y_pred)*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquivo Pickle para implantação\n",
    "\n",
    "Sera gerado um arquivo model.pickle, new_model.pickle, tfidf.pickle\n",
    "\n",
    "pickle é uma biblioteca do Python utilizada para serializar e desserializar objetos Python. Ela permite salvar objetos complexos em arquivos binários que podem ser facilmente carregados novamente posteriormente.\n",
    "\n",
    "No contexto de aprendizado de máquina, é comum salvar modelos treinados e outros objetos importantes para que possam ser reutilizados sem a necessidade de treinamento novamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando a biblioteca pickle do Python para salvar três objetos em arquivos binários, com o propósito de armazenar modelos que foram treinados e criados durante o processo de análise de dados ou aprendizado de máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class_labels\n",
    "label_data = ['email', 'resume' ,'scientific_publication']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando o Modelo de Regressão Logística (model)\n",
    "\n",
    "model é o objeto que representa o modelo treinado de regressão logística.\n",
    "\n",
    "O arquivo \"model.pickle\" conterá todos os parâmetros e configurações do modelo que foram aprendidos durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.pickle\",\"wb\") as file:\n",
    "    pickle.dump(model,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando o Objeto TF-IDF (tfidf)\n",
    "\n",
    "tfidf é o objeto da classe TfidfVectorizer que foi utilizado para transformar os dados textuais em representações numéricas usando TF-IDF.\n",
    "\n",
    "O arquivo \"tfidf.pickle\" contém todos os parâmetros e configurações do objeto tfidf, incluindo as palavras-chave aprendidas durante o ajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tfidf.pickle\",\"wb\") as file1:\n",
    "    pickle.dump(tfidf,file1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Salvando um Novo Modelo (lg_model):\n",
    "\n",
    "lg_model representa o modelo treinado, modelo de aprendizado de máquina de regressão logística\n",
    "\n",
    "O arquivo \"new_model.pickle\" conterá todos os parâmetros e configurações do lg_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_model.pickle\",\"wb\") as file2:\n",
    "    pickle.dump(lg_model,file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação do Modelo\n",
    "\n",
    "Implementar o modelo em um ambiente de produção, o que pode envolver a criação de APIs, integração com sistemas existentes, ou implementação em dashboards.\n",
    "\n",
    "# Interpretação e Comunicação dos Resultados\n",
    "\n",
    "Interpretar os resultados da análise ou modelo e comunicar as conclusões de maneira clara e acessível para os stakeholders, utilizando visualizações e relatórios.\n",
    "\n",
    "# Monitoramento e Manutenção \n",
    "\n",
    "Monitorar o desempenho do modelo em produção e realizar manutenção contínua para assegurar que ele continue a funcionar corretamente e se adaptar a mudanças nos dados ou no ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
