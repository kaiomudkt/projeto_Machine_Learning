{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.2)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2024.5.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de Classes, opções de target\n",
    "class_labels = { 'email':0, 'resume':1, 'scientific_publication':2 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# NLTK (Natural Language Toolkit) \n",
    "# conjunto de palavras stopwords para o idioma inglês \n",
    "# Lista de palavras que são consideradas stopwords (palavras vazias) para o idioma inglês. \n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "# stopwords_list = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download dos recursos necessários do NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_and_remove_special_characters(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converte o texto para minúsculas e remove caracteres especiais.\n",
    "    - Conversão para Minúsculas \n",
    "    - Substituição de Quebras de Linha e Tabulações \n",
    "    - Remoção de Espaços Extras \n",
    "    - Remoção de Números\n",
    "    - Remoção de Pontuação \n",
    "    Args:\n",
    "        text (str): O texto original a ser processado.\n",
    "    Returns:\n",
    "        str: O texto processado com minúsculas e sem caracteres especiais.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokeniza o texto usando NLTK word_tokenize.\n",
    "    Args:\n",
    "        text (str): O texto a ser tokenizado.\n",
    "    Returns:\n",
    "        List[str]: Lista de tokens gerada a partir do texto.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_and_stopwords(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Remove pontuações e stopwords da lista de tokens.\n",
    "    Args:\n",
    "        tokens (List[str]): Lista de tokens a serem processados.\n",
    "    Returns:\n",
    "        List[str]: Lista de tokens sem pontuações e stopwords.\n",
    "    \"\"\"\n",
    "    tokens = [token for token in tokens if token not in punctuation and token not in stopwords_list]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Realiza a lematização dos tokens usando WordNetLemmatizer do NLTK.\n",
    "    Args:\n",
    "        tokens (List[str]): Lista de tokens a serem lematizados.\n",
    "    Returns:\n",
    "        List[str]: Lista de tokens lematizados.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Realiza o pré-processamento completo do texto, incluindo minúsculas,\n",
    "    remoção de caracteres especiais, tokenização, remoção de pontuações\n",
    "    e stopwords, e lematização.\n",
    "    Args:\n",
    "        text (str): O texto original a ser pré-processado.\n",
    "    Returns:\n",
    "        str: O texto pré-processado final, pronto para uso em análises adicionais.\n",
    "    \"\"\"\n",
    "    # Aplica as funções em sequência\n",
    "    text = lowercase_and_remove_special_characters(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = remove_punctuation_and_stopwords(tokens)\n",
    "    final_text = lemmatize_tokens(tokens)\n",
    "    return \" \".join(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "def process_image(label_path: str, file_name: str) -> str:\n",
    "    \"\"\"\n",
    "    - carrega uma imagem.png\n",
    "    - extrai o texto da imagem via OCR\n",
    "    - realiza o pre processamento do texto\n",
    "    retorna o texto ja pre processado\n",
    "    \"\"\"\n",
    "    # Carrega a imagem\n",
    "    image = Image.open(os.path.join(label_path, file_name))\n",
    "    # Usa Tesseract OCR para extrair o texto da imagem\n",
    "    text = pytesseract.image_to_string(image)\n",
    "    # Executa o pré-processamento do texto\n",
    "    processed_text = preprocess_data(text)\n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(path: str)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    retorna DF pandas, com duas colunas 'Text' e 'Label';\n",
    "    'Text': representa os textos extraido via OCR da imagem\n",
    "    'Label' representa a classe/categoria/tipo/label original desta imagem\n",
    "    \"\"\"\n",
    "    # Diretório com as imagens\n",
    "    image_folder = os.listdir(path)\n",
    "    # Lista para armazenar os dados antes de criar o DataFrame\n",
    "    data = []\n",
    "    # Itera sobre cada diretório de classe (label_dir) ['email', 'resume','scientific_publication']\n",
    "    for label_dir in image_folder:\n",
    "        # Caminho completo para o diretório atual de documentos\n",
    "        label_path = os.path.join(path, label_dir)\n",
    "        # Itera sobre os arquivos (imagem.png) dentro do diretório\n",
    "        for file_name in os.listdir(label_path):\n",
    "            processed_text = process_image(label_path, file_name)\n",
    "            # Obtém o rótulo numérico correspondente ao tipo de documento (label)\n",
    "            label = class_labels[label_dir]\n",
    "            # Adiciona um dicionário à lista de dados\n",
    "            data.append({'Text': processed_text, 'Label': label})\n",
    "    # Cria o DataFrame final a partir da lista de dicionários\n",
    "    df = pd.DataFrame(data)        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(path: str, parquet_file: str)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    processa todo o dataset de imagens extensão.png covertando para dataframe pandas já pronto para usar no treinamento do modelo\n",
    "    \"\"\"\n",
    "    # Verifica se o arquivo Parquet já existe\n",
    "    if os.path.exists(parquet_file):\n",
    "        # Carrega os dados existentes se o arquivo Parquet já existir\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "    else:\n",
    "        df = process_images(path)\n",
    "        # Salva o DataFrame em arquivo Parquet para uso futuro não precisar reprocessar e ja pegar pronto\n",
    "        df.to_parquet(parquet_file, index=False)\n",
    "    return df\n",
    "\n",
    "# Armazenar dados processos\n",
    "parquet_file = \"document_texts.parquet\"\n",
    "# Caminho para o diretório contendo as imagens\n",
    "path = \"dataset/www-kaggle-com_datasets_ritvik1909_document-classification-dataset\"\n",
    "# Chamada da função para processar o dataset e obter o DataFrame resultante\n",
    "df = process_dataset(path, parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separando os dados em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vetorizando os documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Configurando o TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(2,5), max_df=0.95, min_df=2 ,max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando e transformando os dados de treinamento\n",
    "tfidf_train = tfidf.fit_transform(X_train)\n",
    "# Convertendo a matriz esparsa em uma matriz densa\n",
    "tfidf_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Lista de modelos para comparar\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de hiperparâmetros usando GridSearchCV\n",
    "\n",
    "busca pela configuração ideal de parâmetros para cada modelo.\n",
    "\n",
    "Definição de param_grids: Para cada modelo, definimos um dicionário de param_grid que contém os parâmetros que desejamos ajustar utilizando GridSearchCV. Os parâmetros específicos foram escolhidos com base na prática comum e podem ser ajustados conforme necessário.\n",
    "\n",
    "Loop de Ajuste de Parâmetros: Iteramos sobre o dicionário models, que contém os modelos a serem testados. Para cada modelo, realizamos o ajuste de hiperparâmetros usando GridSearchCV.\n",
    "\n",
    "GridSearchCV: Criamos um objeto GridSearchCV para cada modelo, onde especificamos o modelo, o param_grid correspondente, o número de folds para validação cruzada (cv=5 neste exemplo) e a métrica de avaliação (scoring='accuracy').\n",
    "\n",
    "Execução da Busca em Grade: Chamamos o método fit() para executar a busca em grade no conjunto de treinamento (tfidf_train, y_train).\n",
    "\n",
    "Armazenamento do Melhor Modelo: Após a busca em grade, armazenamos o melhor modelo encontrado (best_estimator_) no dicionário best_models.\n",
    "\n",
    "Avaliação no Conjunto de Teste: Finalmente, avaliamos os melhores modelos encontrados no conjunto de teste (tfidf_test, y_test) e exibimos a acurácia para cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lr = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [2]\n",
    "}\n",
    "\n",
    "param_grid_svc = {\n",
    "    'C': [1.0],\n",
    "    'kernel': ['linear'],\n",
    "    'gamma': ['scale']\n",
    "}\n",
    "\n",
    "param_grid_nb = {\n",
    "    'alpha': [0.1],\n",
    "    'fit_prior': [True]\n",
    "}\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [10],\n",
    "    'weights': ['uniform'],\n",
    "    'metric': ['euclidean']\n",
    "}\n",
    "\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [200],\n",
    "    'learning_rate': [0.5],\n",
    "    'max_depth': [10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajustando hiperparâmetros para Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros encontrados para Logistic Regression:\n",
      "{'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "\n",
      "Ajustando hiperparâmetros para Random Forest...\n",
      "Melhores parâmetros encontrados para Random Forest:\n",
      "{'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "Ajustando hiperparâmetros para SVM...\n",
      "Melhores parâmetros encontrados para SVM:\n",
      "{'C': 1.0, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\n",
      "Ajustando hiperparâmetros para Naive Bayes...\n",
      "Melhores parâmetros encontrados para Naive Bayes:\n",
      "{'alpha': 0.1, 'fit_prior': True}\n",
      "\n",
      "Ajustando hiperparâmetros para KNN...\n",
      "Melhores parâmetros encontrados para KNN:\n",
      "{'metric': 'euclidean', 'n_neighbors': 10, 'weights': 'uniform'}\n",
      "\n",
      "Ajustando hiperparâmetros para Gradient Boosting...\n",
      "Melhores parâmetros encontrados para Gradient Boosting:\n",
      "{'learning_rate': 0.5, 'max_depth': 10, 'n_estimators': 200}\n",
      "\n",
      "\n",
      "Avaliação dos melhores modelos no conjunto de teste:\n",
      "Logistic Regression: Acurácia no teste = 0.8182\n",
      "Random Forest: Acurácia no teste = 0.9091\n",
      "SVM: Acurácia no teste = 0.8485\n",
      "Naive Bayes: Acurácia no teste = 0.9394\n",
      "KNN: Acurácia no teste = 0.5455\n",
      "Gradient Boosting: Acurácia no teste = 0.8788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir os parâmetros que você deseja testar para cada modelo\n",
    "param_grid_lr = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "param_grid_svc = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "param_grid_nb = {\n",
    "    'alpha': [0.1, 0.5, 1.0],\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 10],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.05, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 10]\n",
    "}\n",
    "\n",
    "# Dicionário de param_grids para cada modelo\n",
    "param_grids = {\n",
    "    'Logistic Regression': param_grid_lr,\n",
    "    'Random Forest': param_grid_rf,\n",
    "    'SVM': param_grid_svc,\n",
    "    'Naive Bayes': param_grid_nb,\n",
    "    'KNN': param_grid_knn,\n",
    "    'Gradient Boosting': param_grid_gb\n",
    "}\n",
    "\n",
    "# Lista para armazenar os melhores modelos ajustados\n",
    "best_models = {}\n",
    "\n",
    "# Executar GridSearchCV para cada modelo\n",
    "for name, model in models.items():\n",
    "    print(f\"Ajustando hiperparâmetros para {name}...\")\n",
    "    \n",
    "    # Definir o param_grid específico para o modelo atual\n",
    "    param_grid = param_grids[name]\n",
    "    \n",
    "    # Criar o objeto GridSearchCV\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Executar a busca em grade no conjunto de treinamento\n",
    "    grid_search.fit(tfidf_train, y_train)\n",
    "    \n",
    "    # Armazenar o melhor modelo ajustado\n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    \n",
    "    # Mostrar os melhores parâmetros encontrados\n",
    "    print(f\"Melhores parâmetros encontrados para {name}:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print()\n",
    "\n",
    "# Avaliar os melhores modelos no conjunto de teste e mostrar métricas\n",
    "print(\"\\nAvaliação dos melhores modelos no conjunto de teste:\")\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    # Avaliar o modelo no conjunto de teste\n",
    "    accuracy = model.score(tfidf_test, y_test)\n",
    "    print(f'{name}: Acurácia no teste = {accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
