{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "import pytesseract\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# NLTK (Natural Language Toolkit) \n",
    "# conjunto de palavras stopwords para o idioma inglês \n",
    "# Lista de palavras que são consideradas stopwords (palavras vazias) para o idioma inglês. \n",
    "stopwords_list = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\n\",\" \").replace(\"\\t\",\" \")\n",
    "    text = re.sub(\"\\s+\",\" \",text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # tokens\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    data = [i for i in tokens if i not in punctuation]\n",
    "    data = [i for i in data if i not in stopwords_list]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    final_text = []\n",
    "    for i in data:\n",
    "        word = lemmatizer.lemmatize(i)\n",
    "        final_text.append(word)\n",
    "        \n",
    "    return \" \".join(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Definição de Classes, opções de target\n",
    "class_labels = {'email':0,'resume':1,'scientific_publication':2}\n",
    "\n",
    "# usada para armazenar os textos extraídos das imagens após o pré-processamento\n",
    "final_text = []\n",
    "# usada para armazenar os rótulos numéricos correspondentes a cada documento\n",
    "final_label = []\n",
    "path = \"dataset/www-kaggle-com_datasets_ritvik1909_document-classification-dataset\"\n",
    "image_folder = os.listdir(path)\n",
    "\n",
    "# Verificar se o arquivo Parquet já existe, para não precisar reprocessar as imagem novamente\n",
    "parquet_file = \"document_texts.parquet\"\n",
    "if os.path.exists(parquet_file):\n",
    "    # Carregar dados existentes se o arquivo Parquet já existir\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    final_text = df['Text'].tolist()\n",
    "    final_label = df['Label'].tolist()\n",
    "else:\n",
    "    # para cada imagem do dataset\n",
    "    for label_dir in image_folder:\n",
    "        # caminho completo para o diretório atual de documentos (por classe 'email', 'resume' e 'scientific_publication')\n",
    "        label_path = os.path.join(path,label_dir)\n",
    "        # Itera sobre os arquivos dentro do diretório \"\"\n",
    "        for j in os.listdir(label_path):\n",
    "            # carrega a imagem.png\n",
    "            image = Image.open(label_path+\"/\"+j)\n",
    "            # Usa Tesseract OCR para extrair o texto da imagem\n",
    "            text = pytesseract.image_to_string(image)\n",
    "            # executa preprocessamento\n",
    "            text_data = preprocess_data(text)\n",
    "            # Adiciona o texto pré-processado (text_data) à lista final_text\n",
    "            final_text.append(text_data)\n",
    "            # Adiciona o rótulo numérico correspondente ao tipo de documento (label) à lista final_label, usando o dicionário class_labels para fazer a correspondência entre nome do diretório e rótulo numérico.\n",
    "            final_label.append(class_labels[label_dir])\n",
    "            \n",
    "    # Criar um DataFrame com os resultados\n",
    "    df = pd.DataFrame({'Text': final_text, 'Label': final_label})\n",
    "    # Salvar DataFrame em arquivo Parquet\n",
    "    df.to_parquet(parquet_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2\n",
       "1      2\n",
       "2      2\n",
       "3      2\n",
       "4      2\n",
       "      ..\n",
       "160    0\n",
       "161    0\n",
       "162    0\n",
       "163    0\n",
       "164    0\n",
       "Name: Label, Length: 165, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando Dataframe pandas com base nas lista 'final_text' e 'final_label'\n",
    "# Coluna 'Text' representa o texto\n",
    "# Coluna 'Label' tem valores 0, 1 e 2 (classes 'email', 'resume' e 'scientific_publication')\n",
    "df = pd.DataFrame()\n",
    "df['Text'] = final_text\n",
    "df['Label'] = final_label\n",
    "df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo o dataframe em treino e teste (split)\n",
    "x_train,x_test,y_train,y_test = train_test_split(df['Text'],df['Label'],shuffle=True,test_size=0.10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Configurando o TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(2,5), max_df=0.95, min_df=2 ,max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Acurácias dos modelos:\n",
      "Logistic Regression: 0.9394\n",
      "Random Forest: 0.9394\n",
      "SVM: 0.9091\n",
      "Naive Bayes: 1.0000\n",
      "KNN: 0.4242\n",
      "Gradient Boosting: 0.8485\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# Vetorizando os documentos\n",
    "tfidf = TfidfVectorizer(ngram_range=(2, 5), max_df=0.95, min_df=2, max_features=10000)\n",
    "tfidf_train = tfidf.fit_transform(df['Text'])\n",
    "\n",
    "# Separando os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_train, df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Lista de modelos para comparar\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Dicionário para armazenar as acurácias\n",
    "accuracies = {}\n",
    "\n",
    "# Treinando e avaliando cada modelo\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies[name] = accuracy\n",
    "\n",
    "print(\"\\nAcurácias dos modelos:\")\n",
    "for name, accuracy in accuracies.items():\n",
    "    print(f'{name}: {accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
