{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# carrega dataset com_datasets_ritvik1909_document-classification-dataset e gera Dataframe pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.2)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2024.5.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de Classes, opções de target\n",
    "class_labels = { 'email':0, 'resume':1, 'scientific_publication':2 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# NLTK (Natural Language Toolkit) \n",
    "# conjunto de palavras stopwords para o idioma inglês \n",
    "# Lista de palavras que são consideradas stopwords (palavras vazias) para o idioma inglês. \n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "# stopwords_list = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download dos recursos necessários do NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_and_remove_special_characters(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Converte o texto para minúsculas e remove caracteres especiais.\n",
    "    - Conversão para Minúsculas \n",
    "    - Substituição de Quebras de Linha e Tabulações \n",
    "    - Remoção de Espaços Extras \n",
    "    - Remoção de Números\n",
    "    - Remoção de Pontuação \n",
    "    Args:\n",
    "        text (str): O texto original a ser processado.\n",
    "    Returns:\n",
    "        str: O texto processado com minúsculas e sem caracteres especiais.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokeniza o texto usando NLTK word_tokenize.\n",
    "    Args:\n",
    "        text (str): O texto a ser tokenizado.\n",
    "    Returns:\n",
    "        List[str]: Lista de tokens gerada a partir do texto.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_and_stopwords(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Remove pontuações e stopwords da lista de tokens.\n",
    "    Args:\n",
    "        tokens (List[str]): Lista de tokens a serem processados.\n",
    "    Returns:\n",
    "        List[str]: Lista de tokens sem pontuações e stopwords.\n",
    "    \"\"\"\n",
    "    tokens = [token for token in tokens if token not in punctuation and token not in stopwords_list]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Realiza a lematização dos tokens usando WordNetLemmatizer do NLTK.\n",
    "    Args:\n",
    "        tokens (List[str]): Lista de tokens a serem lematizados.\n",
    "    Returns:\n",
    "        List[str]: Lista de tokens lematizados.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Realiza o pré-processamento completo do texto, incluindo minúsculas,\n",
    "    remoção de caracteres especiais, tokenização, remoção de pontuações\n",
    "    e stopwords, e lematização.\n",
    "    Args:\n",
    "        text (str): O texto original a ser pré-processado.\n",
    "    Returns:\n",
    "        str: O texto pré-processado final, pronto para uso em análises adicionais.\n",
    "    \"\"\"\n",
    "    # Aplica as funções em sequência\n",
    "    text = lowercase_and_remove_special_characters(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = remove_punctuation_and_stopwords(tokens)\n",
    "    final_text = lemmatize_tokens(tokens)\n",
    "    return \" \".join(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "def process_image(label_path: str, file_name: str) -> str:\n",
    "    \"\"\"\n",
    "    - carrega uma imagem.png\n",
    "    - extrai o texto da imagem via OCR\n",
    "    - realiza o pre processamento do texto\n",
    "    retorna o texto ja pre processado\n",
    "    \"\"\"\n",
    "    # Carrega a imagem\n",
    "    image = Image.open(os.path.join(label_path, file_name))\n",
    "    # Usa Tesseract OCR para extrair o texto da imagem\n",
    "    text = pytesseract.image_to_string(image)\n",
    "    # Executa o pré-processamento do texto\n",
    "    processed_text = preprocess_data(text)\n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(path: str)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    retorna DF pandas, com duas colunas 'Text' e 'Label';\n",
    "    'Text': representa os textos extraido via OCR da imagem\n",
    "    'Label' representa a classe/categoria/tipo/label original desta imagem\n",
    "    \"\"\"\n",
    "    # Diretório com as imagens\n",
    "    image_folder = os.listdir(path)\n",
    "    # Lista para armazenar os dados antes de criar o DataFrame\n",
    "    data = []\n",
    "    # Itera sobre cada diretório de classe (label_dir) ['email', 'resume','scientific_publication']\n",
    "    for label_dir in image_folder:\n",
    "        # Caminho completo para o diretório atual de documentos\n",
    "        label_path = os.path.join(path, label_dir)\n",
    "        # Itera sobre os arquivos (imagem.png) dentro do diretório\n",
    "        for file_name in os.listdir(label_path):\n",
    "            processed_text = process_image(label_path, file_name)\n",
    "            # Obtém o rótulo numérico correspondente ao tipo de documento (label)\n",
    "            label = class_labels[label_dir]\n",
    "            # Adiciona um dicionário à lista de dados\n",
    "            data.append({'Text': processed_text, 'Label': label})\n",
    "    # Cria o DataFrame final a partir da lista de dicionários\n",
    "    df = pd.DataFrame(data)        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(path: str, parquet_file: str)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    processa todo o dataset de imagens extensão.png covertando para dataframe pandas já pronto para usar no treinamento do modelo\n",
    "    \"\"\"\n",
    "    # Verifica se o arquivo Parquet já existe\n",
    "    if os.path.exists(parquet_file):\n",
    "        # Carrega os dados existentes se o arquivo Parquet já existir\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "    else:\n",
    "        df = process_images(path)\n",
    "        # Salva o DataFrame em arquivo Parquet para uso futuro não precisar reprocessar e ja pegar pronto\n",
    "        df.to_parquet(parquet_file, index=False)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Caminho para o diretório contendo as imagens a serem processadas\n",
    "    path = \"dataset/www-kaggle-com_datasets_ritvik1909_document-classification-dataset\"\n",
    "    # apos processar as imagens, Armazenar dados\n",
    "    parquet_file = \"document_texts.parquet\"\n",
    "    # Chamada da função para processar o dataset e obter o DataFrame resultante\n",
    "    df = process_dataset(path, parquet_file)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
