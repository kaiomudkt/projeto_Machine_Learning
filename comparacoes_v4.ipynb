{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mETL_ritvik1909_document_classification_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m main \u001b[38;5;28;01mas\u001b[39;00m get_dataframe\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m get_dataframe()\n",
      "File \u001b[0;32m~/ETL_ritvik1909_document_classification_dataset.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# NLTK (Natural Language Toolkit) \u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m punctuation\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from ETL_ritvik1909_document_classification_dataset import main as get_dataframe\n",
    "df = get_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separando os dados em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vetorizando os documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Configurando o TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(2,5), max_df=0.95, min_df=2 ,max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando e transformando os dados de treinamento\n",
    "tfidf_train = tfidf.fit_transform(X_train)\n",
    "# Convertendo a matriz esparsa em uma matriz densa\n",
    "tfidf_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Lista de modelos para comparar\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de hiperparâmetros usando GridSearchCV\n",
    "\n",
    "busca pela configuração ideal de parâmetros para cada modelo.\n",
    "\n",
    "Definição de param_grids: Para cada modelo, definimos um dicionário de param_grid que contém os parâmetros que desejamos ajustar utilizando GridSearchCV. Os parâmetros específicos foram escolhidos com base na prática comum e podem ser ajustados conforme necessário.\n",
    "\n",
    "Loop de Ajuste de Parâmetros: Iteramos sobre o dicionário models, que contém os modelos a serem testados. Para cada modelo, realizamos o ajuste de hiperparâmetros usando GridSearchCV.\n",
    "\n",
    "GridSearchCV: Criamos um objeto GridSearchCV para cada modelo, onde especificamos o modelo, o param_grid correspondente, o número de folds para validação cruzada (cv=5 neste exemplo) e a métrica de avaliação (scoring='accuracy').\n",
    "\n",
    "Execução da Busca em Grade: Chamamos o método fit() para executar a busca em grade no conjunto de treinamento (tfidf_train, y_train).\n",
    "\n",
    "Armazenamento do Melhor Modelo: Após a busca em grade, armazenamos o melhor modelo encontrado (best_estimator_) no dicionário best_models.\n",
    "\n",
    "Avaliação no Conjunto de Teste: Finalmente, avaliamos os melhores modelos encontrados no conjunto de teste (tfidf_test, y_test) e exibimos a acurácia para cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseado nos melhores parâmetros encontrados\n",
    "param_grid_lr = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [2]\n",
    "}\n",
    "\n",
    "param_grid_svc = {\n",
    "    'C': [1.0],\n",
    "    'kernel': ['linear'],\n",
    "    'gamma': ['scale']\n",
    "}\n",
    "\n",
    "param_grid_nb = {\n",
    "    'alpha': [0.1],\n",
    "    'fit_prior': [True]\n",
    "}\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [10],\n",
    "    'weights': ['uniform'],\n",
    "    'metric': ['euclidean']\n",
    "}\n",
    "\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [200],\n",
    "    'learning_rate': [0.5],\n",
    "    'max_depth': [10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajustando hiperparâmetros para Logistic Regression ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros encontrados para Logistic Regression:\n",
      "{'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "\n",
      "Ajustando hiperparâmetros para Random Forest ...\n",
      "Melhores parâmetros encontrados para Random Forest:\n",
      "{'max_depth': None, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Ajustando hiperparâmetros para SVM ...\n",
      "Melhores parâmetros encontrados para SVM:\n",
      "{'C': 1.0, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "\n",
      "Ajustando hiperparâmetros para Naive Bayes ...\n",
      "Melhores parâmetros encontrados para Naive Bayes:\n",
      "{'alpha': 0.1, 'fit_prior': True}\n",
      "\n",
      "Ajustando hiperparâmetros para KNN ...\n",
      "Melhores parâmetros encontrados para KNN:\n",
      "{'metric': 'euclidean', 'n_neighbors': 10, 'weights': 'uniform'}\n",
      "\n",
      "Ajustando hiperparâmetros para Gradient Boosting ...\n",
      "Melhores parâmetros encontrados para Gradient Boosting:\n",
      "{'learning_rate': 0.5, 'max_depth': 10, 'n_estimators': 200}\n",
      "\n",
      "\n",
      "Avaliação dos melhores modelos no conjunto de teste:\n",
      "Logistic Regression: Acurácia no teste = 0.8182\n",
      "Random Forest: Acurácia no teste = 0.9697\n",
      "SVM: Acurácia no teste = 0.8485\n",
      "Naive Bayes: Acurácia no teste = 0.9394\n",
      "KNN: Acurácia no teste = 0.5455\n",
      "Gradient Boosting: Acurácia no teste = 0.8788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir os primeiros parâmetros para começar a testar em cada modelo\n",
    "param_grid_lr = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "param_grid_svc = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "param_grid_nb = {\n",
    "    'alpha': [0.1, 0.5, 1.0],\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 10],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.05, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 10]\n",
    "}\n",
    "\n",
    "# Dicionário de param_grids para cada modelo\n",
    "param_grids = {\n",
    "    'Logistic Regression': param_grid_lr,\n",
    "    'Random Forest': param_grid_rf,\n",
    "    'SVM': param_grid_svc,\n",
    "    'Naive Bayes': param_grid_nb,\n",
    "    'KNN': param_grid_knn,\n",
    "    'Gradient Boosting': param_grid_gb\n",
    "}\n",
    "\n",
    "# Lista para armazenar os melhores modelos ajustados\n",
    "best_models = {}\n",
    "\n",
    "# Executar GridSearchCV para cada modelo\n",
    "for name, model in models.items():\n",
    "    print(f\"Ajustando hiperparâmetros para {name} ...\")\n",
    "    # Definir o param_grid específico para o modelo atual\n",
    "    param_grid: dict = param_grids[name]\n",
    "    # Criar o objeto GridSearchCV\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "    # Executar a busca em grade no conjunto de treinamento\n",
    "    grid_search.fit(tfidf_train, y_train)    \n",
    "    # Armazenar o melhor modelo ajustado\n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    # Mostrar os melhores parâmetros encontrados\n",
    "    print(f\"Melhores parâmetros encontrados para {name}:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print()\n",
    "\n",
    "# Avaliar os melhores modelos no conjunto de teste e mostrar métricas\n",
    "print(\"\\nAvaliação dos melhores modelos no conjunto de teste:\")\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    # Avaliar o modelo no conjunto de teste\n",
    "    accuracy = model.score(tfidf_test, y_test)\n",
    "    print(f'{name}: Acurácia no teste = {accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
